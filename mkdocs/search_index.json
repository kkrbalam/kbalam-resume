{
    "docs": [
        {
            "location": "/",
            "text": "About Me\n\n\n\n\nMy name is \nKrishna Balam\n and I am a multi-skilled data and business intelligence leader with \n13 years\n of experience in architecting and managing teams for critical data warehouse and business intelligence applications in financial, consumer & retail, insurance and manufacturing sectors.\n\n\nI am very passionate about solving real-world problems with new advents in distributed and cloud computing and also applying technology to help human cause. I am also very interested in latest developments on how data is being used to run political campaigns.\n\n\nI possess a proven ability to manage project teams to successfully deliver agreed upon solutions of the highest quality, on-time, and within budget, often in complex and challenging environments.\n\n\nWhat is my current role?\n\n\nI am currently working as a \nSenior Data Engineering Manager\n at \nCapital One\n. I work in the Analytics Modernization division developing next generation toolset for data scientists and quants community to reduce time to market on machine learning and predictive models from development to production execution.",
            "title": "Home"
        },
        {
            "location": "/#about-me",
            "text": "My name is  Krishna Balam  and I am a multi-skilled data and business intelligence leader with  13 years  of experience in architecting and managing teams for critical data warehouse and business intelligence applications in financial, consumer & retail, insurance and manufacturing sectors.  I am very passionate about solving real-world problems with new advents in distributed and cloud computing and also applying technology to help human cause. I am also very interested in latest developments on how data is being used to run political campaigns.  I possess a proven ability to manage project teams to successfully deliver agreed upon solutions of the highest quality, on-time, and within budget, often in complex and challenging environments.",
            "title": "About Me"
        },
        {
            "location": "/#what-is-my-current-role",
            "text": "I am currently working as a  Senior Data Engineering Manager  at  Capital One . I work in the Analytics Modernization division developing next generation toolset for data scientists and quants community to reduce time to market on machine learning and predictive models from development to production execution.",
            "title": "What is my current role?"
        },
        {
            "location": "/education/",
            "text": "MS in Electrical and Computer Engineering\n\n\n\n\nSchool: \nSDSM&T\n, Rapid City, SD, USA\n\n\nYears: 2003-2005\n\n\nGPA: 4.00\n\n\n\n\n\n\nI have come to the US in 2003 to pursue my masters in Electrical and Computer Engineering with specialization in Very Large Scale Integrated Circuits (VLSI) from South Dakota School of Mines and Technology. Initially it was very diffcult to adopt to teaching techniques followed here, but with help from my mentor and other faculty memebers I was able to overcome these difficulties and graduate with 4.00 GPA. I was able to gain very deep understanding of computer architecture, semi-conductor design and manufacturing processes, Digital Signal Processing, FPGA programming and Nural Networks and Fuzzy logic.\n\n\n\n\nB.Tech in Electronics and Communications Engineering\n\n\n\n\nSchool: \nSNIST\n, Hyderabad, India\n\n\nYears: 1999 - 2003\n\n\nGPA: 3.25\n\n\n\n\n\n\nAfter my high school, in 1999, I went on to join one of the best engineering schools in the region, Sree Nidhi Institute of Science and Technology. I went on to choose electronics and comunications engineering as my major due to my profound interest and fascination towards computers and other electronics devices. I have completed my bachelors with more enthusiasm and eagerness learn about the field.\n\n\n\n\nOther Education\n\n\nI have taken numerous courses from MOOC sites such as \nCoursera\n, \nEdx\n, \nUdacity\n, \nUdemy\n and various bootcamps from Oracle, Hyperion, Informatica, Tableau, Cloudera and DataBricks.",
            "title": "Education"
        },
        {
            "location": "/education/#ms-in-electrical-and-computer-engineering",
            "text": "School:  SDSM&T , Rapid City, SD, USA  Years: 2003-2005  GPA: 4.00    I have come to the US in 2003 to pursue my masters in Electrical and Computer Engineering with specialization in Very Large Scale Integrated Circuits (VLSI) from South Dakota School of Mines and Technology. Initially it was very diffcult to adopt to teaching techniques followed here, but with help from my mentor and other faculty memebers I was able to overcome these difficulties and graduate with 4.00 GPA. I was able to gain very deep understanding of computer architecture, semi-conductor design and manufacturing processes, Digital Signal Processing, FPGA programming and Nural Networks and Fuzzy logic.",
            "title": "MS in Electrical and Computer Engineering"
        },
        {
            "location": "/education/#btech-in-electronics-and-communications-engineering",
            "text": "School:  SNIST , Hyderabad, India  Years: 1999 - 2003  GPA: 3.25    After my high school, in 1999, I went on to join one of the best engineering schools in the region, Sree Nidhi Institute of Science and Technology. I went on to choose electronics and comunications engineering as my major due to my profound interest and fascination towards computers and other electronics devices. I have completed my bachelors with more enthusiasm and eagerness learn about the field.",
            "title": "B.Tech in Electronics and Communications Engineering"
        },
        {
            "location": "/education/#other-education",
            "text": "I have taken numerous courses from MOOC sites such as  Coursera ,  Edx ,  Udacity ,  Udemy  and various bootcamps from Oracle, Hyperion, Informatica, Tableau, Cloudera and DataBricks.",
            "title": "Other Education"
        },
        {
            "location": "/capitalone/",
            "text": "Headquartered in McLean, Virginia, \nCapital One\n offers a broad array of financial products and services to consumers, small businesses and commercial clients in the U.S., Canada, and the UK.\n\n\nI have joined Capital One in October 2015 as a Data Engineering Manager working for Enterprise Data Services, which is a horizontal division that provides technology support across all LOBs in the organization. Below are some of the notable projects I have been part of.\n\n\nMachine Learning Platform Development\n\n\n\n\nDuration:\n December 2016 - Current\n\n\nCapacity:\n Sr. Data Engineering Manager\n\n\nDivision:\n Analytics Modernization\n\n\n\n\n\n\nReporting to Sr. Director, I am leading a team of excellent data & software engineers to build a cutting edge language and framework agnostic machine learning and predictive model development and execution platform in AWS that provides all the modern bells and whistles for model developers and data scientists while reducing the time it takes for a model to get to production. The end goal of the platform is to onboard all existing models in \"legacy\" platforms such as SAS, Teradata, and Oracle.\n\n\n\n\nKey Accomplishments\n\n\n\n\n\n\nv 1.0 of model development product and model execution platform have been released.\n\n\n\n\n\n\nAll packages used are open source, so no software licensing cost involved\n\n\n\n\n\n\nThese products use different cutting edge technologies such as Docker, AWS ECS, EC2, EMR, RDS, Redshift, terraform, Ansible, and Cloudera Express to spin up transient computing for model development and execution\n\n\n\n\n\n\nBy combining DevOps, Cloud and distributed computing solutions, 70% reduction in the time it takes to get a model from development to production\n\n\n\n\n\n\nBy using transient computing and containerization, cost of executing models is reduced already by 60%\n\n\n\n\n\n\nTime spent by Data Science and Quants community for setting up infrastructure, establishing connectivity, instead of model development is now reduced by 75%.\n\n\n\n\n\n\nModernize Coporate Strategy Data Pipelines\n\n\n\n\nDuration:\n October 2015 - December 2016\n\n\nCapacity:\n Data Engineering Manager\n\n\nDivision:\n Enterprise Data Systems - Corporate Strategy\n\n\n\n\n\n\nI was recruited to manage Data Engineering operations for corporate strategy, supporting and enabling upper management to manage data driven strategical and tactical directions for entire organization.\n\n\nAs Data Engineering Manager, my role is responsible for providing data engineering roadmap to upper management, design, develop and maintain Bigdata, data science and decision support solutions through a team of data engineers including offshore vendors.\n\n\n\n\nKey Accomplishments:\n\n\n\n\n\n\nReduced strategical and tactical decision-making time regarding product offerings for upper management by 75%, by providing an ability to analyze, mine and model in-house and competitor data, by implementing brand new scalable and fault tolerant BigData Data Lake for corporate strategy.\n\n\n\n\n\n\n5 million dollars per year in software licensing costs by implementing this data lake on open-source BigData technologies such as Hadoop, Yarn, Python, Spark, and Luigi.\n\n\n\n\n\n\nUnder the process of migrating corporate strategy Data Lake into AWS cloud platform to further reduce costs by 50%, and also provide elasticity for the end user analysis and mining needs.\n\n\n\n\n\n\nHelping peers by guiding them to move their applications from traditional architecture to BigData architecture.\n\n\n\n\n\n\nLeading teams on innovation days to invent new applications for a wide variety of use cases.\n\n\n\n\n\n\nHelped reduce overall costs with an intelligent incorporation of open source technologies and an ideal combination of on-site and offshore developers.",
            "title": "CapitalOne"
        },
        {
            "location": "/capitalone/#machine-learning-platform-development",
            "text": "Duration:  December 2016 - Current  Capacity:  Sr. Data Engineering Manager  Division:  Analytics Modernization    Reporting to Sr. Director, I am leading a team of excellent data & software engineers to build a cutting edge language and framework agnostic machine learning and predictive model development and execution platform in AWS that provides all the modern bells and whistles for model developers and data scientists while reducing the time it takes for a model to get to production. The end goal of the platform is to onboard all existing models in \"legacy\" platforms such as SAS, Teradata, and Oracle.",
            "title": "Machine Learning Platform Development"
        },
        {
            "location": "/capitalone/#key-accomplishments",
            "text": "v 1.0 of model development product and model execution platform have been released.    All packages used are open source, so no software licensing cost involved    These products use different cutting edge technologies such as Docker, AWS ECS, EC2, EMR, RDS, Redshift, terraform, Ansible, and Cloudera Express to spin up transient computing for model development and execution    By combining DevOps, Cloud and distributed computing solutions, 70% reduction in the time it takes to get a model from development to production    By using transient computing and containerization, cost of executing models is reduced already by 60%    Time spent by Data Science and Quants community for setting up infrastructure, establishing connectivity, instead of model development is now reduced by 75%.",
            "title": "Key Accomplishments"
        },
        {
            "location": "/capitalone/#modernize-coporate-strategy-data-pipelines",
            "text": "Duration:  October 2015 - December 2016  Capacity:  Data Engineering Manager  Division:  Enterprise Data Systems - Corporate Strategy    I was recruited to manage Data Engineering operations for corporate strategy, supporting and enabling upper management to manage data driven strategical and tactical directions for entire organization.  As Data Engineering Manager, my role is responsible for providing data engineering roadmap to upper management, design, develop and maintain Bigdata, data science and decision support solutions through a team of data engineers including offshore vendors.",
            "title": "Modernize Coporate Strategy Data Pipelines"
        },
        {
            "location": "/capitalone/#key-accomplishments_1",
            "text": "Reduced strategical and tactical decision-making time regarding product offerings for upper management by 75%, by providing an ability to analyze, mine and model in-house and competitor data, by implementing brand new scalable and fault tolerant BigData Data Lake for corporate strategy.    5 million dollars per year in software licensing costs by implementing this data lake on open-source BigData technologies such as Hadoop, Yarn, Python, Spark, and Luigi.    Under the process of migrating corporate strategy Data Lake into AWS cloud platform to further reduce costs by 50%, and also provide elasticity for the end user analysis and mining needs.    Helping peers by guiding them to move their applications from traditional architecture to BigData architecture.    Leading teams on innovation days to invent new applications for a wide variety of use cases.    Helped reduce overall costs with an intelligent incorporation of open source technologies and an ideal combination of on-site and offshore developers.",
            "title": "Key Accomplishments:"
        },
        {
            "location": "/rosettastone/",
            "text": "Rosetta Stone Inc.\n is dedicated to changing people's lives through the power of language and literacy education. The company's innovative, personalized language and reading programs drive positive learning outcomes in thousands of schools, businesses, government organizations and for millions of individual learners around the world.\n\n\nI was hired in April 2009 to build Enterprise Data Warehousing and Business Intelligence capabilities. RosettaStone was going public and desperately needed these capabilities to be successful.\n\n\nData Warehouse & Business Intelligence COE\n\n\n\n\nDuration:\n April 2009 - Spetember 2015\n\n\nCapacity:\n Data Warehouse & Business Intelligence Architect / Manager\n\n\nDivision:\n Enterprise Data Analytics\n\n\n\n\n\n\nReporting to the CIO, I was recruited to manage Data Warehouse and Data Integration operations for the enterprise and quickly awarded added responsibilities for Business Intelligence when BI Manager left the company.\n\n\nAs Data Warehouse & BI Manager / Architect, my role is responsible for providing enterprise data roadmap to upper management, design, develop and maintain data warehouse and decision support solutions through a team of ETL and BI developers including offshore vendors. My responsibility also included managing the operational budget for internal resources and vendor contracts.\n\n\n\n\nKey Accomplishments\n\n\n\n\n\n\nImplemented Enterprise Data Warehouse (EDW) and Business Intelligence (BI) solutions from ground up to support compliance and analysis requirements, to progress from private to a public company\n\n\n\n\n\n\nStrived to make RosettaStone a data-driven fast paced company, by providing actionable data insights and effective dashboards to various teams in the organization such as, helping supply chain team to reduce order fulfillment time, accounting team to close books quickly, marketing to reduce spending by providing ability to effectively segment and target customers, customer success to communicate with customers effectively based on events and so on\n\n\n\n\n\n\nImplemented Planning and Forecasting application using Hyperion BI Suite for finance team, thereby reducing yearly budgeting effort from 3 months to 3 weeks, monthly forecasting from 2 weeks to 3 days\n\n\n\n\n\n\nHelped transition enterprise strategic and tactical decision-making process from gut feeling to data-driven solutions by providing access to internal and external data sources\n\n\n\n\n\n\nHelped reduce company media spend by 50%, by providing marketers ability to analyze and model data from various media platforms such as Google Analytics and Adwords, Facebook ads, Bing ads, other mobile and TV  ad platforms in single data repository\n\n\n\n\n\n\nHelped boost retention rates from 30% to 75%, and acquisition rates from 5% to 12% by implementing CRM data warehouse using BigData technologies to provide customer 360-degree view to variety of business units such as marketing and customer success to develop more personalized customer interaction and campaign management\n\n\n\n\n\n\nReduced licensing costs by 1.5 million dollars  per year  while providing more reliable, cutting edge and scalable infrastructure and software  for data integration, storage  and data delivery such as Informatica, HP Vertica, Tableau and Hadoop\n\n\n\n\n\n\nSuccessfully deployed onsite-offshore model to offset cost and also provide 24X7 global support for business units around the world",
            "title": "RosettaStone"
        },
        {
            "location": "/rosettastone/#data-warehouse-business-intelligence-coe",
            "text": "Duration:  April 2009 - Spetember 2015  Capacity:  Data Warehouse & Business Intelligence Architect / Manager  Division:  Enterprise Data Analytics    Reporting to the CIO, I was recruited to manage Data Warehouse and Data Integration operations for the enterprise and quickly awarded added responsibilities for Business Intelligence when BI Manager left the company.  As Data Warehouse & BI Manager / Architect, my role is responsible for providing enterprise data roadmap to upper management, design, develop and maintain data warehouse and decision support solutions through a team of ETL and BI developers including offshore vendors. My responsibility also included managing the operational budget for internal resources and vendor contracts.",
            "title": "Data Warehouse &amp; Business Intelligence COE"
        },
        {
            "location": "/rosettastone/#key-accomplishments",
            "text": "Implemented Enterprise Data Warehouse (EDW) and Business Intelligence (BI) solutions from ground up to support compliance and analysis requirements, to progress from private to a public company    Strived to make RosettaStone a data-driven fast paced company, by providing actionable data insights and effective dashboards to various teams in the organization such as, helping supply chain team to reduce order fulfillment time, accounting team to close books quickly, marketing to reduce spending by providing ability to effectively segment and target customers, customer success to communicate with customers effectively based on events and so on    Implemented Planning and Forecasting application using Hyperion BI Suite for finance team, thereby reducing yearly budgeting effort from 3 months to 3 weeks, monthly forecasting from 2 weeks to 3 days    Helped transition enterprise strategic and tactical decision-making process from gut feeling to data-driven solutions by providing access to internal and external data sources    Helped reduce company media spend by 50%, by providing marketers ability to analyze and model data from various media platforms such as Google Analytics and Adwords, Facebook ads, Bing ads, other mobile and TV  ad platforms in single data repository    Helped boost retention rates from 30% to 75%, and acquisition rates from 5% to 12% by implementing CRM data warehouse using BigData technologies to provide customer 360-degree view to variety of business units such as marketing and customer success to develop more personalized customer interaction and campaign management    Reduced licensing costs by 1.5 million dollars  per year  while providing more reliable, cutting edge and scalable infrastructure and software  for data integration, storage  and data delivery such as Informatica, HP Vertica, Tableau and Hadoop    Successfully deployed onsite-offshore model to offset cost and also provide 24X7 global support for business units around the world",
            "title": "Key Accomplishments"
        },
        {
            "location": "/qimonda/",
            "text": "Qimonda\n designs semiconductor memory technologies, as well as develops, manufactures, markets, and sells various semiconductor memory products on a chip, component, and module level.\n\n\nI have joined a semiconductor manufacturing company called Qimonda NA in May 2008, thinking that my course work in Electrical Engineering and my experience in computer science is a perfect combination for an exciting experience.\n\n\nManufacturing Business Intelligence Practice\n\n\n\n\nDuration:\n May 2008 - March 2009\n\n\nCapacity:\n Sr. Information Systems Engineer\n\n\nDivision:\n Manufacturing Business Intelligence\n\n\n\n\n\n\nHired as a Sr. Information Systems Engineer to be a part of manufacturing business intelligence team, my role was to manage and guide a team of Junior Information Systems Engineers, to design a new near real-time replication mechanism between factory transaction databases and data warehouse, design and develop master data management initiatives, employ Lean Six Sigma methodology in data warehouse design, development and maintenance activities, 24X7 support for Data Warehouses and as well as custom designed reporting applications.\n\n\nMy responsibilities included applying my deep experience in database and data warehouse development to provide low latency near real-time insights for line managers all over the globe as well as upper management to effectively manage and reduce glitches in manufacturing. I also had the responsibility of providing a master data repository to all business units around the world for process parameters to streamline new product development.\n\n\n\n\nKey Accomplishments\n\n\n\n\n\n\nImplemented near real-time data replication process (using Oracle MV logs and fast refresh) to migrate data from factory databases into enterprise data warehouse to provide near real-time insights for manufacturing line managers about the status and thereby reducing the number of manufacturing line stoppages by 75%.\n\n\n\n\n\n\nProvided data governance capabilities, improved data quality, by implementing a master data repository called FIPS (fabrication incorporates planning and simulation) to standardize process parameters across all factories around the world to streamline the new product manufacturing process and to reduce time to market.\n\n\n\n\n\n\nIntegrated data from factory databases and ERP systems spread across the world and provided reports and dashboards with manufacturing intelligence to a wide variety of business units.",
            "title": "Qimonda NA"
        },
        {
            "location": "/qimonda/#manufacturing-business-intelligence-practice",
            "text": "Duration:  May 2008 - March 2009  Capacity:  Sr. Information Systems Engineer  Division:  Manufacturing Business Intelligence    Hired as a Sr. Information Systems Engineer to be a part of manufacturing business intelligence team, my role was to manage and guide a team of Junior Information Systems Engineers, to design a new near real-time replication mechanism between factory transaction databases and data warehouse, design and develop master data management initiatives, employ Lean Six Sigma methodology in data warehouse design, development and maintenance activities, 24X7 support for Data Warehouses and as well as custom designed reporting applications.  My responsibilities included applying my deep experience in database and data warehouse development to provide low latency near real-time insights for line managers all over the globe as well as upper management to effectively manage and reduce glitches in manufacturing. I also had the responsibility of providing a master data repository to all business units around the world for process parameters to streamline new product development.",
            "title": "Manufacturing Business Intelligence Practice"
        },
        {
            "location": "/qimonda/#key-accomplishments",
            "text": "Implemented near real-time data replication process (using Oracle MV logs and fast refresh) to migrate data from factory databases into enterprise data warehouse to provide near real-time insights for manufacturing line managers about the status and thereby reducing the number of manufacturing line stoppages by 75%.    Provided data governance capabilities, improved data quality, by implementing a master data repository called FIPS (fabrication incorporates planning and simulation) to standardize process parameters across all factories around the world to streamline the new product manufacturing process and to reduce time to market.    Integrated data from factory databases and ERP systems spread across the world and provided reports and dashboards with manufacturing intelligence to a wide variety of business units.",
            "title": "Key Accomplishments"
        },
        {
            "location": "/jpmc/",
            "text": "JPMorgan Chase\n  is one of the oldest financial institutions in the United States. With a history dating back over 200 years, helping customers and communities worldwide.\n\n\nI have joined JPMorgan Chase as a Data Warehouse Developer Analyst in October 2005 to help with ETL and Data Warehouse development activities as part of Chase and Bank one merger. Below is the summary of my experience:\n\n\nPrivate Label Technology Services (PLTS)\n\n\n\n\nDuration:\n October 2005 - May 2008\n\n\nCapacity:\n Data Warehouse Analyst IV\n\n\nDivision:\n Private Label Technology Services\n\n\n\n\n\n\nI was hired as a data warehouse developer for Private Label Technology Services (PLTS) that supported Circuit City private label and co-branded Visa, United Airlines and Chase co-branded and Health Care Finance portfolios.\n\n\nDuring this time I was involved in launching new private label clients in the existing environment, managing offshore employees, Run Time Engine (RTE) tasks, 24X7 production support and also tactical and strategical enhancements to the warehouse, infrastructure related tech refreshes, tuning and harnessing the warehouse for analytical reporting, designing and deploying data marts, and assisting in the daily flash reports, monthly yield reports and yearly yield reports generation for each portfolio.\n\n\nMy responsibility was to design, develop and maintain sound data warehouse and data integration solutions for PLTS data warehouse that were optimized, scalable and maintainable.\n\n\n\n\nKey Accomplishments:\n\n\n\n\n\n\nHelped reduce migration time by 50% for Chase enterprise credit card data warehouse team, to migrate and integrate PLTS data into the enterprise card data warehouse, serving as an SME for PLTS data and processes.\n\n\n\n\n\n\nHelped save 1.5 million, by developing a calculation engine for customer rewards points expiration in collaboration with reward point vendor Maritz, based on business rules.\n\n\n\n\n\n\nHelped reduce fraud by 30%, by implementing Fraud DataMart to calculate and analyze fraud scores for newly entered credit card applications for credit cards owned by PLTS.\n\n\n\n\n\n\nDesigned, developed and deployed a calculation and reporting engine to fulfill a program agreement to share fees based on the catalyst for new applications that were written between Circuit City and PLTS to promote new accounts.\n\n\n\n\n\n\nWorked on migrating PLTS data warehouse from Richmond to Chase enterprise data center, during which ETL tool Informatica was upgraded from 5.1 to 7.1.4, the operating system from HP-UX to AIX and database from Oracle 9i to Oracle 10g.",
            "title": "JPMorgan Chase"
        },
        {
            "location": "/jpmc/#private-label-technology-services-plts",
            "text": "Duration:  October 2005 - May 2008  Capacity:  Data Warehouse Analyst IV  Division:  Private Label Technology Services    I was hired as a data warehouse developer for Private Label Technology Services (PLTS) that supported Circuit City private label and co-branded Visa, United Airlines and Chase co-branded and Health Care Finance portfolios.  During this time I was involved in launching new private label clients in the existing environment, managing offshore employees, Run Time Engine (RTE) tasks, 24X7 production support and also tactical and strategical enhancements to the warehouse, infrastructure related tech refreshes, tuning and harnessing the warehouse for analytical reporting, designing and deploying data marts, and assisting in the daily flash reports, monthly yield reports and yearly yield reports generation for each portfolio.  My responsibility was to design, develop and maintain sound data warehouse and data integration solutions for PLTS data warehouse that were optimized, scalable and maintainable.",
            "title": "Private Label Technology Services (PLTS)"
        },
        {
            "location": "/jpmc/#key-accomplishments",
            "text": "Helped reduce migration time by 50% for Chase enterprise credit card data warehouse team, to migrate and integrate PLTS data into the enterprise card data warehouse, serving as an SME for PLTS data and processes.    Helped save 1.5 million, by developing a calculation engine for customer rewards points expiration in collaboration with reward point vendor Maritz, based on business rules.    Helped reduce fraud by 30%, by implementing Fraud DataMart to calculate and analyze fraud scores for newly entered credit card applications for credit cards owned by PLTS.    Designed, developed and deployed a calculation and reporting engine to fulfill a program agreement to share fees based on the catalyst for new applications that were written between Circuit City and PLTS to promote new accounts.    Worked on migrating PLTS data warehouse from Richmond to Chase enterprise data center, during which ETL tool Informatica was upgraded from 5.1 to 7.1.4, the operating system from HP-UX to AIX and database from Oracle 9i to Oracle 10g.",
            "title": "Key Accomplishments:"
        },
        {
            "location": "/rhtinfoway/",
            "text": "RHT Infoway Inc. is an IT consulting services company that helps customers fulfill their project needs.\n\n\nI have joined RHT Infoway in December 2004 as a CPT, helping customers with ETL and Data Warehouse development challenges. As part of this position, below are the projects that I have worked on:\n\n\nEastern Region Data Store Development\n\n\n\n\nClient:\n First Health\n\n\nDuration:\n March 2005 - September 2005\n\n\nCapacity:\n ETL Developer\n\n\nDivision:\n Eastern Region Data Store\n\n\n\n\n\n\nI was hired as an ETL developer to perform data integration tasks for eastern region data store creation. My role was to apply sound ETL development techniques and best practices to integrate data from heterogeneous sources.\n\n\n\n\nKey Accomplishments\n\n\n\n\n\n\nDeveloped complete set of ETL mappings for all dimensions and fact tables in Eastern Region Data Store containing data for subscribers who live in the eastern region of the United States for detailed analysis.\n\n\n\n\n\n\nProvided run books and production support manuals for production support team to address any issues with ETL processes.\n\n\n\n\n\n\nHousing Datamart Development\n\n\n\n\nClient:\n Electric Insurance\n\n\nDuration:\n December 2004 - March 2005\n\n\nCapacity:\n ETL Developer\n\n\nDivision:\n Housing insurance\n\n\n\n\n\n\nI was hired as an ETL contractor to help develop, test and deploy data extraction, transformation processes required to create housing data mart.\n\n\n\n\nKey Accomplishments\n\n\n\n\n\n\nDesigned, developed and deployed ETL processes required to load housing insurance data from enterprise data warehouse into newly created housing data mart to support department-specific reporting.\n\n\n\n\n\n\nProvided documentation and training to associates for maintaining these processes.",
            "title": "RHT Infoway"
        },
        {
            "location": "/rhtinfoway/#eastern-region-data-store-development",
            "text": "Client:  First Health  Duration:  March 2005 - September 2005  Capacity:  ETL Developer  Division:  Eastern Region Data Store    I was hired as an ETL developer to perform data integration tasks for eastern region data store creation. My role was to apply sound ETL development techniques and best practices to integrate data from heterogeneous sources.",
            "title": "Eastern Region Data Store Development"
        },
        {
            "location": "/rhtinfoway/#key-accomplishments",
            "text": "Developed complete set of ETL mappings for all dimensions and fact tables in Eastern Region Data Store containing data for subscribers who live in the eastern region of the United States for detailed analysis.    Provided run books and production support manuals for production support team to address any issues with ETL processes.",
            "title": "Key Accomplishments"
        },
        {
            "location": "/rhtinfoway/#housing-datamart-development",
            "text": "Client:  Electric Insurance  Duration:  December 2004 - March 2005  Capacity:  ETL Developer  Division:  Housing insurance    I was hired as an ETL contractor to help develop, test and deploy data extraction, transformation processes required to create housing data mart.",
            "title": "Housing Datamart Development"
        },
        {
            "location": "/rhtinfoway/#key-accomplishments_1",
            "text": "Designed, developed and deployed ETL processes required to load housing insurance data from enterprise data warehouse into newly created housing data mart to support department-specific reporting.    Provided documentation and training to associates for maintaining these processes.",
            "title": "Key Accomplishments"
        },
        {
            "location": "/sdsmt/",
            "text": "Founded in 1885 to provide instruction in the region\u2019s primary industry, mining, the \nSouth Dakota School of Mines & Technology\n is a thriving science and engineering research university today, boasting sixteen academic departments.\n\n\nWhile pursuing MS in Electrical & Computer Engineering, I was hired as a graduate assistant in administrative office to support data analysis requests that are requested by different departments in the University. Below is the project that I have worked on:\n\n\nStudent Information System\n\n\n\n\nDuration:\n August 2003 - December 2004\n\n\nCapacity:\n Graduate Assistant\n\n\nDivision:\n Administrative Office\n\n\n\n\n\n\nDeveloped data integration and data analysis processes for management committee as well as other departments to serve their data needs.\n\n\n\n\nKey Accomplishments\n\n\n\n\n\n\nDeveloped code for enhancements to the data warehouse that was hosting data from PeopleSoft system.\n\n\n\n\n\n\nDeveloped automated reporting for student rating and various other trends.",
            "title": "SDSM&T"
        },
        {
            "location": "/sdsmt/#student-information-system",
            "text": "Duration:  August 2003 - December 2004  Capacity:  Graduate Assistant  Division:  Administrative Office    Developed data integration and data analysis processes for management committee as well as other departments to serve their data needs.",
            "title": "Student Information System"
        },
        {
            "location": "/sdsmt/#key-accomplishments",
            "text": "Developed code for enhancements to the data warehouse that was hosting data from PeopleSoft system.    Developed automated reporting for student rating and various other trends.",
            "title": "Key Accomplishments"
        },
        {
            "location": "/expertise/",
            "text": "I am a multi-skilled data engineering and business intelligence leader with 13+ years of experience in managing teams and architecting solutions for critical and actionable data products in consumer & retail, financial and manufacturing industries.\n\n\nCore Competencies\n\n\n\n\n\n\nHighly experienced in managing and leading teams, providing data-driven solutions to business problems in variety of business units such as accounting, finance, supply chain, inventory, product development, marketing and customer support\n\n\n\n\n\n\nHands on experience in architecting and implementing decision support, business intelligence, data mining and machine learning solutions in Big Data (Hadoop, Kafka, Hive, Spark and Yarn etc.), cloud (AWS EC2, EMR, RDS, Redshift, and ECS) and traditional data warehouse platforms\n\n\n\n\n\n\nExpertise in solution architecture for converting legacy in-house data stores and products into scalable, distributed, near real-time cloud hosted solutions \n\n\n\n\n\n\nDelivering solutions using out-sourcing model managing offshore and onshore partners\n\n\n\n\n\n\nDetailed and practical experience of data warehouse methodologies (such as Kimball, Inmon and Data Vault) and frameworks (such as Zachman\u2019s Framework)\n\n\n\n\n\n\nAbility to perform multiple vendor evaluations to choose the best solution for business problems\n\n\n\n\n\n\nDetailed understanding of data warehouse and big data infrastructures such as Amazon Redshift, HP Vertica, and Paraccel. ETL tools such as Informatica, Oracle Data Integrator, Pentaho DI, Talend, Informatica Cloud & Spark\n\n\n\n\n\n\nDetailed experience in building data warehouses from ground up on databases such as Oracle, SQL Server, MySQL, Vertica and DB2\n\n\n\n\n\n\nExpert level experience in business intelligence tools such as OBIEE, Tableau, Cognos and Hyperion\n\n\n\n\n\n\nVery good programming knowledge in SQL, PL/SQL, Shell Scripting, Hyperion Scripts, Python, C, Scala, and Matlab",
            "title": "Expertise"
        },
        {
            "location": "/expertise/#core-competencies",
            "text": "Highly experienced in managing and leading teams, providing data-driven solutions to business problems in variety of business units such as accounting, finance, supply chain, inventory, product development, marketing and customer support    Hands on experience in architecting and implementing decision support, business intelligence, data mining and machine learning solutions in Big Data (Hadoop, Kafka, Hive, Spark and Yarn etc.), cloud (AWS EC2, EMR, RDS, Redshift, and ECS) and traditional data warehouse platforms    Expertise in solution architecture for converting legacy in-house data stores and products into scalable, distributed, near real-time cloud hosted solutions     Delivering solutions using out-sourcing model managing offshore and onshore partners    Detailed and practical experience of data warehouse methodologies (such as Kimball, Inmon and Data Vault) and frameworks (such as Zachman\u2019s Framework)    Ability to perform multiple vendor evaluations to choose the best solution for business problems    Detailed understanding of data warehouse and big data infrastructures such as Amazon Redshift, HP Vertica, and Paraccel. ETL tools such as Informatica, Oracle Data Integrator, Pentaho DI, Talend, Informatica Cloud & Spark    Detailed experience in building data warehouses from ground up on databases such as Oracle, SQL Server, MySQL, Vertica and DB2    Expert level experience in business intelligence tools such as OBIEE, Tableau, Cognos and Hyperion    Very good programming knowledge in SQL, PL/SQL, Shell Scripting, Hyperion Scripts, Python, C, Scala, and Matlab",
            "title": "Core Competencies"
        },
        {
            "location": "/skills/",
            "text": "Areas of Expertise\n\n\n\n\nData Engineering\n\n\nCloud Solution Architecture\n\n\nDevOps\n\n\nContainerized Solution Design\n\n\nBigData Analytics\n\n\nBusiness Intelligence\n\n\nDistributed Architecture\n\n\nData Warehouse Development\n\n\nETL Development\n\n\nDatabase Development\n\n\nMachine Learning\n\n\nSolutions Design\n\n\n\n\nTechnical Skills\n\n\n\n\n\n\n\n\nArea\n\n\nTechnologies\n\n\n\n\n\n\n\n\n\n\nData Engineering\n\n\nSpark, Scala, Python, ETL, SQL, PL/SQL, Luigi\n\n\n\n\n\n\nCloud\n\n\nAWS S3, EC2, EMR, ECS, RDS, RedShift, CloudFormation, IAM, DigitalOcean, Heroku, Azure\n\n\n\n\n\n\nDevOps\n\n\nTerraform, Ansible, CloudFormation, Docker, Vagrant\n\n\n\n\n\n\nContainerization\n\n\nDocker, Docker Compose, Docker Swarm, Docker Stack, Vagrant, ECS\n\n\n\n\n\n\nBigData Analytics\n\n\nHive, Impala, SparkSQL, Tableau\n\n\n\n\n\n\nBusiness Intelligence\n\n\nOBIEE, Tableau, Matplotlib, Plotly, Cognos\n\n\n\n\n\n\nDistributed Computing\n\n\nHadoop, Yarn, HDFS, MapReduce, Spark, Yarn, EMR, Cloudera CDH\n\n\n\n\n\n\nETL Development\n\n\nSpark, Python, Scala, Apache NiFi, Informatica PowerCenter, Talend, SQL, PL/SQL, Shell Scripts\n\n\n\n\n\n\nDatabases\n\n\nAWS RedShift, HP Vertica, Oracle, MySQL, Postgres, Hive, SQL Server, Redis\n\n\n\n\n\n\nProgramming Languages\n\n\nC, C++, Python, Scala, Ruby, Julia, R, Clojure\n\n\n\n\n\n\n\n\nPersonal Skills\n\n\n\n\nFocused\n\n\nAttention to detail\n\n\nGoals orientated\n\n\nPresentation skills\n\n\nLeadership skills\n\n\nProject management\n\n\nProblem-solving skills\n\n\nOffshore management skills",
            "title": "Skills"
        },
        {
            "location": "/skills/#areas-of-expertise",
            "text": "Data Engineering  Cloud Solution Architecture  DevOps  Containerized Solution Design  BigData Analytics  Business Intelligence  Distributed Architecture  Data Warehouse Development  ETL Development  Database Development  Machine Learning  Solutions Design",
            "title": "Areas of Expertise"
        },
        {
            "location": "/skills/#technical-skills",
            "text": "Area  Technologies      Data Engineering  Spark, Scala, Python, ETL, SQL, PL/SQL, Luigi    Cloud  AWS S3, EC2, EMR, ECS, RDS, RedShift, CloudFormation, IAM, DigitalOcean, Heroku, Azure    DevOps  Terraform, Ansible, CloudFormation, Docker, Vagrant    Containerization  Docker, Docker Compose, Docker Swarm, Docker Stack, Vagrant, ECS    BigData Analytics  Hive, Impala, SparkSQL, Tableau    Business Intelligence  OBIEE, Tableau, Matplotlib, Plotly, Cognos    Distributed Computing  Hadoop, Yarn, HDFS, MapReduce, Spark, Yarn, EMR, Cloudera CDH    ETL Development  Spark, Python, Scala, Apache NiFi, Informatica PowerCenter, Talend, SQL, PL/SQL, Shell Scripts    Databases  AWS RedShift, HP Vertica, Oracle, MySQL, Postgres, Hive, SQL Server, Redis    Programming Languages  C, C++, Python, Scala, Ruby, Julia, R, Clojure",
            "title": "Technical Skills"
        },
        {
            "location": "/skills/#personal-skills",
            "text": "Focused  Attention to detail  Goals orientated  Presentation skills  Leadership skills  Project management  Problem-solving skills  Offshore management skills",
            "title": "Personal Skills"
        },
        {
            "location": "/contact/",
            "text": "Contact Me\n\n\n\n\nE-mail:\n krishna.Balam@gmail.com\n\n\nGithub:\n kkrbalam\n\n\nLinkedIn:\n https://www.linkedin.com/in/krishnakbalam\n\n\nTwitter:\n @KrishnaBalam",
            "title": "Contact Me"
        },
        {
            "location": "/contact/#contact-me",
            "text": "E-mail:  krishna.Balam@gmail.com  Github:  kkrbalam  LinkedIn:  https://www.linkedin.com/in/krishnakbalam  Twitter:  @KrishnaBalam",
            "title": "Contact Me"
        }
    ]
}